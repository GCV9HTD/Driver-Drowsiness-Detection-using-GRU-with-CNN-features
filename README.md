# Driver-Drowsiness-Detection-using-GRU-with-CNN-features

This algorithm detects driver drowsiness detection using Deep learning concepts like , CNN, RNN, GRU.
We use yawning as our basis

## Workflow

1)We extract frames from incoming video stream using openCV.
2)Extracted frames are sent to custom dlib shape predictor to predict mouth region and extract it.
2)We train our custom dlib shape predictor as guided in this well written article https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/

3)Cropped mouth region in sent in the features extractor, which extracts required spatial features.
4)Finally, features are concatenated in sets of 32 and are sent to yawn detector for yawning classification.

## Custom dlib shape predictor 
Custom dlib shape predictor has been trained on [i-bug 300W](https://ibug.doc.ic.ac.uk/resources/300-W/) using steps provided [here](https://www.pyimagesearch.com/2019/12/16/training-a-custom-dlib-shape-predictor/)

Custom dlib shape predictor folder contains the following file.

1)parse_xml.py : Parses the train/test XML dataset files for eyes-only landmark coordinates.

2)train_shape_predictor.py : Accepts the parsed XML files to train our shape predictor with dlib, saves custom shape predictor with .dat file extension.

3)evaluate_shape_predictor.py : Calculates the Mean Average Error (MAE) of our custom shape predictor to test and validate our model.

For running the files use

`python train_shape_predictor.py --training 'path/to/.xml/file/comtaining/mouth/only/landmarks' --model 'path/to/model.dat'`

`python evaluate_shape_predictor.py --predictor 'path/to/trained/shape_predictor' --xml 'path/to/.xml/file/comtaining/mouth/only/landmarks'`

`python parse_xml.py --input 'path/to/input/.xml/file'  --output 'path/to/output/.xml/file'`

## Feature Extractor

Feature Extractor was trained on a selected number of images from [AffectNet](http://mohammadmahoor.com/affectnet/) dataset.

1)feature_extractor_train.py: trains the model and saves it in .h5 format.

Then, [keras-surgeon](https://github.com/BenWhetton/keras-surgeon) was used to remove the last dense layer used for classification so we can get features generated by second last dense layer.

[YawDD](https://www.researchgate.net/publication/262255270_YawDD_A_yawning_detection_dataset) dataset was used to train our yawn detector, before that frames of yawning and not yawning were extracted and sent to our feature extractor. The extracted features were then saved and used to train our yawn detector.

## Yawn Detector

Yawn detector was trained on the features extracted from YawDD dataset using feature extractor.


## Real time inference
Real time inference can be done using model_32_to_1.py file present in folder Real Time Inference.

Requirements:
1)Custom dlib shape predictor
2)Pre-trained feature extractor
3)Pre-trained yawn detector

Commnad Line `python model_github.py --shape-predictor path/to/shape/predictor --input_method 1 or 0`

## Dependencies


## Run it on your own 
1)Clone the repository
2)
